{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c810fd2d-67c1-427a-a58d-244265ead069",
   "metadata": {},
   "source": [
    "#### Car Dekho Price Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3188b310-35dd-4e4f-b39f-6aad4c05b536",
   "metadata": {},
   "source": [
    "##### Converting Unstructred dataset to Structed dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d489fb81-fb25-4e5f-9934-57fb95dd13b4",
   "metadata": {},
   "source": [
    "Bangalore_Cars_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29056459-0a11-472e-93b9-1071ea0ab3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Saved\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# Load your Excel file\n",
    "df = pd.read_excel(r\"C:\\Project_Guvi\\Capstone3\\bangalore_cars.xlsx\")\n",
    "\n",
    "# Function to flatten nested dictionaries \n",
    "def flatten(d, parent_key='', sep='_'):\n",
    "    items = []\n",
    "    for k, v in d.items():\n",
    "        new_key = f'{parent_key}{sep}{k}' if parent_key else k\n",
    "        if isinstance(v, dict):\n",
    "            items.extend(flatten(v, new_key, sep=sep).items())\n",
    "        elif isinstance(v, list):\n",
    "            for i, item in enumerate(v):\n",
    "                if isinstance(item, dict):\n",
    "                    items.extend(flatten(item, f'{new_key}_{i}', sep=sep).items())\n",
    "                else:\n",
    "                    items.append((f'{new_key}_{i}', item))\n",
    "        else:\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)\n",
    "\n",
    "# Function to process and flatten a specified column in the DataFrame\n",
    "def process_column(df, column_name):\n",
    "    flattened_data = []\n",
    "    for row in df[column_name]:\n",
    "        try:\n",
    "            unstructured_data = ast.literal_eval(row)\n",
    "            flattened_row = flatten(unstructured_data)\n",
    "        except (ValueError, SyntaxError):\n",
    "            flattened_row = {}\n",
    "        flattened_data.append(flattened_row)\n",
    "    return pd.DataFrame(flattened_data)\n",
    "\n",
    "\n",
    "new_car_details_df = process_column(df, 'new_car_detail')\n",
    "new_car_overview_df = process_column(df, 'new_car_overview')\n",
    "new_car_features_df = process_column(df, 'new_car_feature')\n",
    "new_car_specs_df = process_column(df, 'new_car_specs')\n",
    "\n",
    "# Copy non-nested data(car_links)\n",
    "new_car_link_df = df[['car_links']].copy()\n",
    "\n",
    "# Merge all structured DataFrames into one\n",
    "structured_data = pd.concat([\n",
    "    new_car_details_df, \n",
    "    new_car_overview_df, \n",
    "    new_car_features_df, \n",
    "    new_car_specs_df, \n",
    "    new_car_link_df\n",
    "], axis=1)\n",
    "\n",
    "# Add a 'City' column \n",
    "structured_data['City'] = 'Bangalore'\n",
    "\n",
    "output_path = \"C:\\\\Project_Guvi\\\\Capstone3\\\\Structured_data\\\\bangalore_cars_structured.csv\"\n",
    "# Save the merged DataFrame to a CSV file\n",
    "structured_data.to_csv(output_path, index=False)\n",
    "print(\"File Saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab3f64c-509c-46a7-8a98-70ba938c4233",
   "metadata": {},
   "source": [
    "Chennai_Cars_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "122df49f-f3b0-4ca5-a626-049e0b30886d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Saved\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# Load your Excel file\n",
    "df = pd.read_excel(r\"C:\\Project_Guvi\\Capstone3\\chennai_cars.xlsx\")\n",
    "\n",
    "# Function to flatten nested dictionaries or lists\n",
    "def flatten(d, parent_key='', sep='_'):\n",
    "    items = []\n",
    "    for k, v in d.items():\n",
    "        new_key = f'{parent_key}{sep}{k}' if parent_key else k\n",
    "        if isinstance(v, dict):\n",
    "            items.extend(flatten(v, new_key, sep=sep).items())\n",
    "        elif isinstance(v, list):\n",
    "            for i, item in enumerate(v):\n",
    "                if isinstance(item, dict):\n",
    "                    items.extend(flatten(item, f'{new_key}_{i}', sep=sep).items())\n",
    "                else:\n",
    "                    items.append((f'{new_key}_{i}', item))\n",
    "        else:\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)\n",
    "\n",
    "# Function to process and flatten a specified column in the DataFrame\n",
    "def process_column(df, column_name):\n",
    "    flattened_data = []\n",
    "    for row in df[column_name]:\n",
    "        try:\n",
    "            unstructured_data = ast.literal_eval(row)\n",
    "            flattened_row = flatten(unstructured_data)\n",
    "        except (ValueError, SyntaxError):\n",
    "            flattened_row = {}\n",
    "        flattened_data.append(flattened_row)\n",
    "    return pd.DataFrame(flattened_data)\n",
    "\n",
    "new_car_details_df = process_column(df, 'new_car_detail')\n",
    "new_car_overview_df = process_column(df, 'new_car_overview')\n",
    "new_car_features_df = process_column(df, 'new_car_feature')\n",
    "new_car_specs_df = process_column(df, 'new_car_specs')\n",
    "\n",
    "# copy non-nested data(car_links)\n",
    "new_car_link_df = df[['car_links']].copy()\n",
    "\n",
    "# Merge all structured DataFrames into one\n",
    "structured_data = pd.concat([\n",
    "    new_car_details_df, \n",
    "    new_car_overview_df, \n",
    "    new_car_features_df, \n",
    "    new_car_specs_df, \n",
    "    new_car_link_df\n",
    "], axis=1)\n",
    "\n",
    "# Add a 'City' column \n",
    "structured_data['City'] = 'Chennai'\n",
    "\n",
    "output_path = 'C:\\\\Project_Guvi\\\\Capstone3\\\\Structured_data\\\\chennai_cars_Structured.csv'\n",
    "# Save the merged DataFrame to a CSV file\n",
    "structured_data.to_csv(output_path, index=False)\n",
    "print(\"File Saved\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8623ade6-b456-407e-a4d2-1524f8fba24e",
   "metadata": {},
   "source": [
    "Delhi_Cars_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a632802a-65bd-4c86-9067-29f8c80c78cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Saved\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# Load your Excel file\n",
    "df = pd.read_excel(r\"C:\\Project_Guvi\\Capstone3\\delhi_cars.xlsx\")\n",
    "\n",
    "# Function to flatten nested dictionaries or lists\n",
    "def flatten(d, parent_key='', sep='_'):\n",
    "    items = []\n",
    "    for k, v in d.items():\n",
    "        new_key = f'{parent_key}{sep}{k}' if parent_key else k\n",
    "        if isinstance(v, dict):\n",
    "            items.extend(flatten(v, new_key, sep=sep).items())\n",
    "        elif isinstance(v, list):\n",
    "            for i, item in enumerate(v):\n",
    "                if isinstance(item, dict):\n",
    "                    items.extend(flatten(item, f'{new_key}_{i}', sep=sep).items())\n",
    "                else:\n",
    "                    items.append((f'{new_key}_{i}', item))\n",
    "        else:\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)\n",
    "\n",
    "# Function to process and flatten a specified column in the DataFrame\n",
    "def process_column(df, column_name):\n",
    "    flattened_data = []\n",
    "    for row in df[column_name]:\n",
    "        try:\n",
    "            unstructured_data = ast.literal_eval(row)\n",
    "            flattened_row = flatten(unstructured_data)\n",
    "        except (ValueError, SyntaxError):\n",
    "            flattened_row = {}\n",
    "        flattened_data.append(flattened_row)\n",
    "    return pd.DataFrame(flattened_data)\n",
    "\n",
    "new_car_details_df = process_column(df, 'new_car_detail')\n",
    "new_car_overview_df = process_column(df, 'new_car_overview')\n",
    "new_car_features_df = process_column(df, 'new_car_feature')\n",
    "new_car_specs_df = process_column(df, 'new_car_specs')\n",
    "\n",
    "# Directly copy non-nested data\n",
    "new_car_link_df = df[['car_links']].copy()\n",
    "\n",
    "# Merge all structured DataFrames into one\n",
    "structured_data = pd.concat([\n",
    "    new_car_details_df, \n",
    "    new_car_overview_df, \n",
    "    new_car_features_df, \n",
    "    new_car_specs_df, \n",
    "    new_car_link_df\n",
    "], axis=1)\n",
    "\n",
    "# Add a 'City' column \n",
    "structured_data['City'] = 'Delhi'\n",
    "\n",
    "output_path = 'C:\\\\Project_Guvi\\\\Capstone3\\\\Structured_data\\\\delhi_cars_Structured.csv'\n",
    "# Save the merged DataFrame to a CSV file\n",
    "structured_data.to_csv(output_path, index=False)\n",
    "print(\"File Saved\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7588c691-501e-4501-978c-8c62aa9b4baa",
   "metadata": {},
   "source": [
    "Hyderabad_Cars_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e6eb62f-590e-4b83-b9af-1c890cc1f61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Saved\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# Load your Excel file\n",
    "df = pd.read_excel(r\"C:\\Project_Guvi\\Capstone3\\hyderabad_cars.xlsx\")\n",
    "\n",
    "# Function to flatten nested dictionaries or lists\n",
    "def flatten(d, parent_key='', sep='_'):\n",
    "    items = []\n",
    "    for k, v in d.items():\n",
    "        new_key = f'{parent_key}{sep}{k}' if parent_key else k\n",
    "        if isinstance(v, dict):\n",
    "            items.extend(flatten(v, new_key, sep=sep).items())\n",
    "        elif isinstance(v, list):\n",
    "            for i, item in enumerate(v):\n",
    "                if isinstance(item, dict):\n",
    "                    items.extend(flatten(item, f'{new_key}_{i}', sep=sep).items())\n",
    "                else:\n",
    "                    items.append((f'{new_key}_{i}', item))\n",
    "        else:\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)\n",
    "\n",
    "# Function to process and flatten a specified column in the DataFrame\n",
    "def process_column(df, column_name):\n",
    "    flattened_data = []\n",
    "    for row in df[column_name]:\n",
    "        try:\n",
    "            unstructured_data = ast.literal_eval(row)\n",
    "            flattened_row = flatten(unstructured_data)\n",
    "        except (ValueError, SyntaxError):\n",
    "            flattened_row = {}\n",
    "        flattened_data.append(flattened_row)\n",
    "    return pd.DataFrame(flattened_data)\n",
    "\n",
    "\n",
    "new_car_details_df = process_column(df, 'new_car_detail')\n",
    "new_car_overview_df = process_column(df, 'new_car_overview')\n",
    "new_car_features_df = process_column(df, 'new_car_feature')\n",
    "new_car_specs_df = process_column(df, 'new_car_specs')\n",
    "\n",
    "# Directly copy non-nested data\n",
    "new_car_link_df = df[['car_links']].copy()\n",
    "\n",
    "# Merge all structured DataFrames into one\n",
    "structured_data = pd.concat([\n",
    "    new_car_details_df, \n",
    "    new_car_overview_df, \n",
    "    new_car_features_df, \n",
    "    new_car_specs_df, \n",
    "    new_car_link_df\n",
    "], axis=1)\n",
    "\n",
    "# Add a 'City' column\n",
    "structured_data['City'] = 'Hyderabad'\n",
    "\n",
    "\n",
    "output_csv_path = 'C:\\\\Project_Guvi\\\\Capstone3\\\\Structured_data\\\\hyderabad_cars_Structured.csv'\n",
    "# Save the merged DataFrame to a CSV file\n",
    "structured_data.to_csv(output_csv_path, index=False)\n",
    "print(\"File Saved\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948593af-412f-4029-887e-fe002d58151a",
   "metadata": {},
   "source": [
    "Jaipur_Cars_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a498f38-1375-4b51-b0f8-dad5c38a6f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Saved\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# Load your Excel file\n",
    "df = pd.read_excel(\"C:\\\\Project_Guvi\\\\Capstone3\\\\jaipur_cars.xlsx\")\n",
    "\n",
    "# Function to flatten nested dictionaries or lists\n",
    "def flatten(d, parent_key='', sep='_'):\n",
    "    items = []\n",
    "    for k, v in d.items():\n",
    "        new_key = f'{parent_key}{sep}{k}' if parent_key else k\n",
    "        if isinstance(v, dict):\n",
    "            items.extend(flatten(v, new_key, sep=sep).items())\n",
    "        elif isinstance(v, list):\n",
    "            for i, item in enumerate(v):\n",
    "                if isinstance(item, dict):\n",
    "                    items.extend(flatten(item, f'{new_key}_{i}', sep=sep).items())\n",
    "                else:\n",
    "                    items.append((f'{new_key}_{i}', item))\n",
    "        else:\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)\n",
    "\n",
    "# Function to process and flatten a specified column in the DataFrame\n",
    "def process_column(df, column_name):\n",
    "    flattened_data = []\n",
    "    for row in df[column_name]:\n",
    "        try:\n",
    "            unstructured_data = ast.literal_eval(row)\n",
    "            flattened_row = flatten(unstructured_data)\n",
    "        except (ValueError, SyntaxError):\n",
    "            flattened_row = {}\n",
    "        flattened_data.append(flattened_row)\n",
    "    return pd.DataFrame(flattened_data)\n",
    "\n",
    "new_car_details_df = process_column(df, 'new_car_detail')\n",
    "new_car_overview_df = process_column(df, 'new_car_overview')\n",
    "new_car_features_df = process_column(df, 'new_car_feature')\n",
    "new_car_specs_df = process_column(df, 'new_car_specs')\n",
    "\n",
    "# Directly copy non-nested data\n",
    "new_car_link_df = df[['car_links']].copy()\n",
    "\n",
    "# Merge all structured DataFrames into one\n",
    "structured_data = pd.concat([\n",
    "    new_car_details_df, \n",
    "    new_car_overview_df, \n",
    "    new_car_features_df, \n",
    "    new_car_specs_df, \n",
    "    new_car_link_df\n",
    "], axis=1)\n",
    "\n",
    "# Add a 'City' column to indicate the location\n",
    "structured_data['City'] = 'Jaipur'\n",
    "\n",
    "output_csv_path = 'C:\\\\Project_Guvi\\\\Capstone3\\\\Structured_data\\\\jaipur_cars_Structured.csv'\n",
    "# Save the merged DataFrame to a CSV file\n",
    "structured_data.to_csv(output_csv_path, index=False)\n",
    "print(\"File Saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1a775d-eeec-4c20-bffc-6ea0039d8da3",
   "metadata": {},
   "source": [
    "Kolkata_Cars_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b63a9c5e-08c1-42ad-8dd4-dd904139ec3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Saved\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# Load your Excel file\n",
    "df = pd.read_excel(r\"C:\\Project_Guvi\\Capstone3\\kolkata_cars.xlsx\")\n",
    "\n",
    "# Function to flatten nested dictionaries or lists\n",
    "def flatten(d, parent_key='', sep='_'):\n",
    "    items = []\n",
    "    for k, v in d.items():\n",
    "        new_key = f'{parent_key}{sep}{k}' if parent_key else k\n",
    "        if isinstance(v, dict):\n",
    "            items.extend(flatten(v, new_key, sep=sep).items())\n",
    "        elif isinstance(v, list):\n",
    "            for i, item in enumerate(v):\n",
    "                if isinstance(item, dict):\n",
    "                    items.extend(flatten(item, f'{new_key}_{i}', sep=sep).items())\n",
    "                else:\n",
    "                    items.append((f'{new_key}_{i}', item))\n",
    "        else:\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)\n",
    "\n",
    "# Function to process and flatten a specified column in the DataFrame\n",
    "def process_column(df, column_name):\n",
    "    flattened_data = []\n",
    "    for row in df[column_name]:\n",
    "        try:\n",
    "            unstructured_data = ast.literal_eval(row)\n",
    "            flattened_row = flatten(unstructured_data)\n",
    "        except (ValueError, SyntaxError):\n",
    "            flattened_row = {}\n",
    "        flattened_data.append(flattened_row)\n",
    "    return pd.DataFrame(flattened_data)\n",
    "\n",
    "new_car_details_df = process_column(df, 'new_car_detail')\n",
    "new_car_overview_df = process_column(df, 'new_car_overview')\n",
    "new_car_features_df = process_column(df, 'new_car_feature')\n",
    "new_car_specs_df = process_column(df, 'new_car_specs')\n",
    "\n",
    "# Directly copy non-nested data\n",
    "new_car_link_df = df[['car_links']].copy()\n",
    "\n",
    "# Merge all structured DataFrames into one\n",
    "structured_data = pd.concat([\n",
    "    new_car_details_df, \n",
    "    new_car_overview_df, \n",
    "    new_car_features_df, \n",
    "    new_car_specs_df, \n",
    "    new_car_link_df\n",
    "], axis=1)\n",
    "\n",
    "# Add a 'City' column \n",
    "structured_data['City'] = 'Kolkata'\n",
    "\n",
    "output_csv_path = 'C:\\\\Project_Guvi\\\\Capstone3\\\\Structured_data\\\\kolkata_cars_Structured.csv'\n",
    "# Save the merged DataFrame to a CSV file\n",
    "structured_data.to_csv(output_csv_path, index=False)\n",
    "print(\"File Saved\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1def99d-ec15-47eb-a02e-42cac3868eee",
   "metadata": {},
   "source": [
    "Concatenating All CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b2db7e4-0148-42ed-a1a5-bd019515de68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Dataset Concatenated and Saved\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# list of Structured csv files path\n",
    "csv_files_path =[\n",
    "    \"C:\\\\Project_Guvi\\\\Capstone3\\\\Structured_data\\\\bangalore_cars_Structured.csv\",\n",
    "    \"C:\\\\Project_Guvi\\\\Capstone3\\\\Structured_data\\\\chennai_cars_Structured.csv\",\n",
    "    \"C:\\\\Project_Guvi\\\\Capstone3\\\\Structured_data\\\\delhi_cars_Structured.csv\",\n",
    "    \"C:\\\\Project_Guvi\\\\Capstone3\\\\Structured_data\\\\hyderabad_cars_Structured.csv\",\n",
    "    \"C:\\\\Project_Guvi\\\\Capstone3\\\\Structured_data\\\\jaipur_cars_Structured.csv\",\n",
    "    \"C:\\\\Project_Guvi\\\\Capstone3\\\\Structured_data\\\\kolkata_cars_Structured.csv\"\n",
    "]\n",
    "\n",
    "# List to Store dataframes\n",
    "dfs = []\n",
    "\n",
    "# Loop \n",
    "for file in csv_files_path:\n",
    "    df = pd.read_csv(file)\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all Dataframes\n",
    "all_data_df = pd.concat(dfs, ignore_index = True)\n",
    "\n",
    "# save the file\n",
    "file_path = \"C:\\Project_Guvi\\Capstone3\\Structured_data\\Structured_Cars_data.csv\"\n",
    "all_data_df.to_csv(file_path,index = False)\n",
    "print(\"All Dataset Concatenated and Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a4e1ab-1d81-435a-b475-5764db54a03f",
   "metadata": {},
   "source": [
    "##### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c684aff-aa63-4a57-847d-569a37dd9900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Cleaned and File Saved\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load your csv file\n",
    "df = pd.read_csv(r\"C:\\Project_Guvi\\Capstone3\\Structured_data\\Structured_Cars_data.csv\",low_memory=False)\n",
    "\n",
    "# Calculate the threshold for dropping columns (50% missing values)\n",
    "threshold = len(df) * 0.5\n",
    "\n",
    "# Drop columns with more than 50% missing values\n",
    "df = df.dropna(thresh=threshold, axis=1)\n",
    "\n",
    "# Remove commas and convert the 'km' column to integers\n",
    "df['km'] = df['km'].str.replace(',', '').astype(int)\n",
    "\n",
    "# Function to format 'price' column\n",
    "def clean_price_column(price):\n",
    "    price = price.replace('₹', '').replace(',', '').strip()\n",
    "    if 'Lakh' in price:\n",
    "        price = float(price.replace('Lakh', '').strip()) * 1e5\n",
    "    elif 'Crore' in price:\n",
    "        price = float(price.replace('Crore', '').strip()) * 1e7\n",
    "    else:\n",
    "        price = float(price)\n",
    "    return price\n",
    "\n",
    "# Apply the function to 'price' column\n",
    "df['price'] = df['price'].apply(clean_price_column)\n",
    "\n",
    "# Function to format the 'top_3_value(it contains seats data)' column\n",
    "def clean_seat_column(value):\n",
    "    if 'Kms' in str(value):\n",
    "        return np.nan  \n",
    "    elif 'Seats' in str(value):\n",
    "        try:\n",
    "            seats = int(value.replace('Seats', '').strip())\n",
    "            return seats\n",
    "        except ValueError:\n",
    "            return np.nan  \n",
    "    else:\n",
    "        return np.nan \n",
    "\n",
    "# Apply the function to 'top_3_value' \n",
    "df['seats'] = df['top_3_value'].apply(clean_seat_column)\n",
    "\n",
    "# Fill missing values in the seats column with the mean\n",
    "df['seats'].fillna(df['seats'].mean(), inplace=True)\n",
    "\n",
    "# Function to clean mileage values\n",
    "def clean_mileage(value):\n",
    "    if 'kmpl' in value:\n",
    "        return float(value.replace(' kmpl', '').strip())\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Apply the function to the 'mileage' column\n",
    "df['mileage'] = df['top_0_value.2'].apply(clean_mileage)\n",
    "\n",
    "# Calculate the median of the mileage column\n",
    "median_mileage = df['mileage'].median()\n",
    "\n",
    "# Fill missing values in the mileage column with the median\n",
    "df['mileage'].fillna(median_mileage, inplace=True)\n",
    "\n",
    "# drop original column of 'seats' and 'mileage\n",
    "df.drop(['top_0_value.2', 'top_3_value'], axis=1, inplace=True)\n",
    "\n",
    "# drop 'owner' column it same as ownerNo\n",
    "df.drop(columns=['owner'], inplace=True)\n",
    "\n",
    "# drop 'it' column it has same value\n",
    "df.drop(columns=['it'], inplace = True)\n",
    "\n",
    "# Calculate Q1 and Q3 for the 'Price' column\n",
    "Q1 = df['price'].quantile(0.25)\n",
    "Q3 = df['price'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# lower and upper bounds for outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Filter out the outliers\n",
    "df = df[(df['price'] >= lower_bound) & (df['price'] <= upper_bound)]\n",
    "\n",
    "# Save the cleaned dataset\n",
    "df.to_csv('C:\\\\Project_Guvi\\\\Capstone3\\\\Cleaned_Car_Dataset_Raw.csv', index = False)\n",
    "print(\"Data Cleaned and File Saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee42aae-c1af-4206-835d-6ce997f7b359",
   "metadata": {},
   "source": [
    "##### Data Cleaning With Scaling and Ecoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb7c5ff8-72e8-48d7-a46a-47ac5808b9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Cleaned and File Saved\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "import joblib\n",
    "\n",
    "# Load your csv file\n",
    "df = pd.read_csv(r\"C:\\Project_Guvi\\Capstone3\\Structured_data\\Structured_Cars_data.csv\",low_memory=False)\n",
    "\n",
    "# Calculate the threshold for dropping columns (50% missing values)\n",
    "threshold = len(df) * 0.5\n",
    "\n",
    "# Drop columns with more than 50% missing values\n",
    "df = df.dropna(thresh=threshold, axis=1)\n",
    "\n",
    "# Remove commas and convert the 'km' column to integers\n",
    "df['km'] = df['km'].str.replace(',', '').astype(int)\n",
    "\n",
    "# Function to format 'price' column\n",
    "def clean_price_column(price):\n",
    "    price = price.replace('₹', '').replace(',', '').strip()\n",
    "    if 'Lakh' in price:\n",
    "        price = float(price.replace('Lakh', '').strip()) * 1e5\n",
    "    elif 'Crore' in price:\n",
    "        price = float(price.replace('Crore', '').strip()) * 1e7\n",
    "    else:\n",
    "        price = float(price)\n",
    "    return price\n",
    "\n",
    "# Apply the function to 'price' column\n",
    "df['price'] = df['price'].apply(clean_price_column)\n",
    "\n",
    "# Function to format the 'top_3_value(it contains seats data)' column\n",
    "def clean_seat_column(value):\n",
    "    if 'Kms' in str(value):\n",
    "        return np.nan  \n",
    "    elif 'Seats' in str(value):\n",
    "        try:\n",
    "            seats = int(value.replace('Seats', '').strip())\n",
    "            return seats\n",
    "        except ValueError:\n",
    "            return np.nan  \n",
    "    else:\n",
    "        return np.nan \n",
    "\n",
    "# Apply the function to 'top_3_value' \n",
    "df['seats'] = df['top_3_value'].apply(clean_seat_column)\n",
    "\n",
    "# Fill missing values in the seats column with the mean\n",
    "df['seats'].fillna(df['seats'].mean(), inplace=True)\n",
    "\n",
    "# Function to clean mileage values\n",
    "def clean_mileage(value):\n",
    "    if 'kmpl' in value:\n",
    "        return float(value.replace(' kmpl', '').strip())\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Apply the function to the 'mileage' column\n",
    "df['mileage'] = df['top_0_value.2'].apply(clean_mileage)\n",
    "\n",
    "# Calculate the median of the mileage column\n",
    "median_mileage = df['mileage'].median()\n",
    "\n",
    "# Fill missing values in the mileage column with the median\n",
    "df['mileage'].fillna(median_mileage, inplace=True)\n",
    "\n",
    "# drop original column of 'seats' and 'mileage\n",
    "df.drop(['top_0_value.2', 'top_3_value'], axis=1, inplace=True)\n",
    "\n",
    "# drop 'owner' column it same as ownerNo\n",
    "df.drop(columns=['owner'], inplace=True)\n",
    "\n",
    "# drop 'it' column it has same value\n",
    "df.drop(columns=['it'], inplace = True)\n",
    "\n",
    "# Lable encoding for categorical columns\n",
    "def label_encode(df, categorical_columns):\n",
    "    label_encoder = LabelEncoder()\n",
    "    for column in categorical_columns:\n",
    "        if column in df.columns:\n",
    "            df[column] = label_encoder.fit_transform(df[column].astype(str))\n",
    "    return df\n",
    "\n",
    "categorical_columns = ['ft','bt', 'transmission','oem','model','variantName','City'] \n",
    "\n",
    "# Apply the label encoding function\n",
    "df_encoded = label_encode(df, categorical_columns)\n",
    "\n",
    "def min_max_scaling(df, numeric_columns):\n",
    "    scaler = MinMaxScaler()\n",
    "    for column in numeric_columns:\n",
    "        if column in df.columns:\n",
    "            df[column] = scaler.fit_transform(df[[column]])\n",
    "    return df\n",
    "\n",
    "numeric_columns =  ['km','modelYear','ownerNo','mileage','seats']\n",
    "\n",
    "# Apply the Min-Max scaling function\n",
    "df_scaled = min_max_scaling(df, numeric_columns)\n",
    "\n",
    "# Calculate Q1 and Q3 for the 'Price' column\n",
    "Q1 = df['price'].quantile(0.25)\n",
    "Q3 = df['price'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# lower and upper bounds for outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Filter out the outliers\n",
    "df = df[(df['price'] >= lower_bound) & (df['price'] <= upper_bound)]\n",
    "\n",
    "# save the cleaned data\n",
    "df.to_csv('C:\\\\Project_Guvi\\\\Capstone3\\\\Cleaned_Car_Dataset.csv', index = False)\n",
    "\n",
    "# Save the scaled and encoded DataFrame to a pkl file \n",
    "joblib.dump(df_scaled, 'C:\\\\Project_Guvi\\\\Capstone3\\\\scaled.pkl')\n",
    "joblib.dump(df_encoded, 'C:\\\\Project_Guvi\\\\Capstone3\\\\lable_encoded.pkl')\n",
    "\n",
    "print(\"Data Cleaned and File Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a4d692-695f-4b0e-ac47-50b50542f686",
   "metadata": {},
   "source": [
    "##### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea4e7c01-b3ba-4dbd-835e-a0d9eec7a56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression CV Mean MSE: 47392436690.2329\n",
      "Linear Regression - MSE: 44970393717.99427, MAE: 149746.60683519553, R²: 0.6340391643989778\n",
      "Best Ridge Alpha: {'alpha': 0.1}\n",
      "Best Lasso Alpha: {'alpha': 100}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"C:\\\\Project_Guvi\\\\Capstone3\\\\Cleaned_Car_Dataset.csv\",low_memory=False)\n",
    "\n",
    "# Define features and target\n",
    "X = df[['ft', 'bt', 'km', 'transmission', 'ownerNo', 'oem', 'model', 'modelYear', 'variantName', 'City', 'mileage', 'seats']]\n",
    "y = df['price']\n",
    "\n",
    "# Train Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Model Training\n",
    "lr_regressor = LinearRegression()\n",
    "lr_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Cross-Validation\n",
    "cv_scores = cross_val_score(lr_regressor, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "print(f'Linear Regression CV Mean MSE: {-cv_scores.mean()}')\n",
    "\n",
    "# Model Prediction\n",
    "y_pred = lr_regressor.predict(X_test)\n",
    "\n",
    "# Model Evaluation\n",
    "mse_lr = mean_squared_error(y_test, y_pred)\n",
    "mae_lr = mean_absolute_error(y_test, y_pred)\n",
    "r2_lr = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Linear Regression - MSE: {mse_lr}, MAE: {mae_lr}, R²: {r2_lr}')\n",
    "\n",
    "# Hyperparameter Tuning for Ridge and Lasso using Grid Search\n",
    "ridge = Ridge()\n",
    "ridge_params = {'alpha': [0.01, 0.1, 1, 10, 100]}\n",
    "ridge_grid = GridSearchCV(ridge, ridge_params, cv=5, scoring='neg_mean_squared_error')\n",
    "ridge_grid.fit(X_train, y_train)\n",
    "print(f'Best Ridge Alpha: {ridge_grid.best_params_}')\n",
    "\n",
    "lasso = Lasso()\n",
    "lasso_params = {'alpha': [0.01, 0.1, 1, 10, 100]}\n",
    "lasso_grid = GridSearchCV(lasso, lasso_params, cv=5, scoring='neg_mean_squared_error')\n",
    "lasso_grid.fit(X_train, y_train)\n",
    "print(f'Best Lasso Alpha: {lasso_grid.best_params_}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c118ba-be0d-449d-a3f3-d54bd9568874",
   "metadata": {},
   "source": [
    "##### Decision Tree with Cross-Validation and Hyperparameter Tuning (Grid Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a615c46-9e5c-4cac-a08c-ab6654626c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree CV Mean MSE: 43475365689.4325\n",
      "Decision Tree - MSE: 40255689345.9193, MAE: 146357.9138, R²: 0.6724\n",
      "Best Decision Tree Params: {'max_depth': 20, 'min_samples_leaf': 4, 'min_samples_split': 10}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"C:\\\\Project_Guvi\\\\Capstone3\\\\Cleaned_Car_Dataset.csv\", low_memory=False)\n",
    "\n",
    "# Define features and target\n",
    "X = df[['ft', 'bt', 'km', 'transmission', 'ownerNo', 'oem', 'model', 'modelYear', 'variantName', 'City', 'mileage', 'seats']]\n",
    "y = df['price']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Model training \n",
    "dtree = DecisionTreeRegressor(random_state=42, max_depth=5)\n",
    "dtree.fit(X_train, y_train)\n",
    "\n",
    "# Cross-Validation\n",
    "dtree_cv_scores = cross_val_score(dtree, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "print(f'Decision Tree CV Mean MSE: {-dtree_cv_scores.mean():.4f}')\n",
    "\n",
    "# Predictions\n",
    "y_pred_dtree = dtree.predict(X_test)\n",
    "\n",
    "# Model Evaluation\n",
    "mse_dt = mean_squared_error(y_test, y_pred_dtree)\n",
    "mae_dt = mean_absolute_error(y_test, y_pred_dtree)\n",
    "r2_dt = r2_score(y_test, y_pred_dtree)\n",
    "\n",
    "print(f'Decision Tree - MSE: {mse_dt:.4f}, MAE: {mae_dt:.4f}, R²: {r2_dt:.4f}')\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [10, 20, 30, 40], \n",
    "    'min_samples_split': [2, 5, 10],      \n",
    "    'min_samples_leaf': [1, 2, 4]      \n",
    "}\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV\n",
    "dt_grid = GridSearchCV(DecisionTreeRegressor(random_state=42), param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "dt_grid.fit(X_train, y_train)\n",
    "\n",
    "# Best hyperparameters \n",
    "best_params = dt_grid.best_params_\n",
    "print(f'Best Decision Tree Params: {best_params}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4360960d-9dcb-47b5-97b8-29a03c7706ef",
   "metadata": {},
   "source": [
    "##### Gradient Boosting with Cross-Validation and Hyperparameter Tuning (Random Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be9f50dd-5880-484a-84cc-6ca10e9be32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting CV Mean MSE: 21325205781.40378\n",
      "Gradient Boosting - MSE: 20095984467.69358, MAE: 100327.61351, R²: 0.83646\n",
      "Best Gradient Boosting Params: {'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': 7, 'learning_rate': 0.1}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"C:\\\\Project_Guvi\\\\Capstone3\\\\Cleaned_Car_Dataset.csv\", low_memory=False)\n",
    "\n",
    "# Define features and target\n",
    "X = df[['ft', 'bt', 'km', 'transmission', 'ownerNo', 'oem', 'model', 'modelYear', 'variantName', 'City', 'mileage', 'seats']]\n",
    "y = df['price']\n",
    " \n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Model training \n",
    "gboost = GradientBoostingRegressor(random_state=42)\n",
    "gboost.fit(X_train, y_train)\n",
    "\n",
    "# Cross-Validation\n",
    "gboost_cv_scores = cross_val_score(gboost, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "print(f'Gradient Boosting CV Mean MSE: {-gboost_cv_scores.mean():.5f}')\n",
    "\n",
    "# Predictions\n",
    "y_pred_gboost = gboost.predict(X_test)\n",
    "\n",
    "# Model Evaluation\n",
    "mse_gb = mean_squared_error(y_test, y_pred_gboost)\n",
    "mae_gb = mean_absolute_error(y_test, y_pred_gboost)\n",
    "r2_gb = r2_score(y_test, y_pred_gboost)\n",
    "\n",
    "print(f'Gradient Boosting - MSE: {mse_gb:.5f}, MAE: {mae_gb:.5f}, R²: {r2_gb:.5f}')\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],  \n",
    "    'learning_rate': [0.01, 0.1, 0.2],  \n",
    "    'max_depth': [3, 5, 7],             \n",
    "    'min_samples_split': [2, 5, 10],    \n",
    "    'min_samples_leaf': [1, 2, 4]    \n",
    "}\n",
    "\n",
    "# Hyperparameter tuning using RandomizedSearchCV\n",
    "gboost_random = RandomizedSearchCV(GradientBoostingRegressor(random_state=42), param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "gboost_random.fit(X_train, y_train)\n",
    "\n",
    "# Best hyperparameters\n",
    "best_params_gboost = gboost_random.best_params_\n",
    "print(f'Best Gradient Boosting Params: {best_params_gboost}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c26c29-2380-4753-85bb-e529ec7eb0a0",
   "metadata": {},
   "source": [
    "##### Random Forest with Cross-Validation and Hyperparameter Tuning (Random Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a3d83e0b-7ddd-469c-9679-78f95ce4dbc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest CV Mean MSE: 15958977022.39484\n",
      "Random Forest - MSE: 14932338543.74129, MAE: 77836.24598, R²: 0.87848\n",
      "Best Random Forest Params: {'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': 30}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"C:\\\\Project_Guvi\\\\Capstone3\\\\Cleaned_Car_Dataset.csv\", low_memory=False)\n",
    "\n",
    "# Define features and target\n",
    "X = df[['ft', 'bt', 'km', 'transmission', 'ownerNo', 'oem', 'model', 'modelYear', 'variantName', 'City', 'mileage', 'seats']]\n",
    "y = df['price']\n",
    " \n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Model training\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Cross-Validation\n",
    "rf_cv_scores = cross_val_score(rf, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "print(f'Random Forest CV Mean MSE: {-rf_cv_scores.mean():.5f}')\n",
    "\n",
    "# Predictions\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# Model Evaluation\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "print(f'Random Forest - MSE: {mse_rf:.5f}, MAE: {mae_rf:.5f}, R²: {r2_rf:.5f}')\n",
    "\n",
    "# Hyperparameter Tuning using Random Search\n",
    "rf_params = {\n",
    "    'n_estimators': [100, 200, 300, 400],\n",
    "    'max_depth': [10, 20, 30, 40],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "rf_random = RandomizedSearchCV(rf, rf_params, n_iter=20, cv=5, scoring='neg_mean_squared_error', random_state=42)\n",
    "rf_random.fit(X_train, y_train)\n",
    "\n",
    "print(f'Best Random Forest Params: {rf_random.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5654bedb-111e-4b35-a53b-c9017aab9985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Comparison:\n",
      "               Model           MSE            MAE        R²\n",
      "0  Linear Regression  4.497039e+10  149746.606835  0.634039\n",
      "1      Random Forest  1.493234e+10   77836.245977  0.878483\n",
      "2  Gradient Boosting  2.009598e+10  100327.613511  0.836463\n",
      "3      Decision Tree  4.025569e+10  146357.913775  0.672407\n",
      "\n",
      "Best Model Summary:\n",
      "Best Model: Random Forest\n",
      "MSE: 14932338543.741291\n",
      "MAE: 77836.245977\n",
      "R²: 0.878483\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = {\n",
    "    \"Model\": [\"Linear Regression\", \"Random Forest\", \"Gradient Boosting\", \"Decision Tree\"],\n",
    "    \"MSE\": [mse_lr, mse_rf, mse_gb, mse_dt],\n",
    "    \"MAE\": [mae_lr, mae_rf, mae_gb, mae_dt],\n",
    "    \"R²\": [r2_lr, r2_rf, r2_gb, r2_dt]\n",
    "}\n",
    "\n",
    "# Create a DataFrame to hold the results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the model comparison table\n",
    "print(\"Model Comparison:\")\n",
    "print(results_df)\n",
    "\n",
    "# Find the model with the highest R²\n",
    "best_r2_model = results_df.loc[results_df['R²'].idxmax()]\n",
    "\n",
    "best_model = results_df[\n",
    "    (results_df['R²'] == best_r2_model['R²']) &\n",
    "    (results_df['MSE'] == results_df['MSE'].min()) &\n",
    "    (results_df['MAE'] == results_df['MAE'].min())\n",
    "]\n",
    "\n",
    "best_model = best_model.iloc[0]\n",
    "\n",
    "print(\"\\nBest Model Summary:\")\n",
    "print(f\"Best Model: {best_model['Model']}\")\n",
    "print(f\"MSE: {best_model['MSE']:.6f}\")\n",
    "print(f\"MAE: {best_model['MAE']:.6f}\")\n",
    "print(f\"R²: {best_model['R²']:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35827fdc-9b44-4676-90d6-0ac31090f68f",
   "metadata": {},
   "source": [
    "##### Finalized Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bdf4021-6777-48a9-9947-d99a0681064c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest CV Mean MSE: 15535795563.48110\n",
      "Random Forest - MSE: 13361432289.139915, MAE: 74128.27336791146, R²: 0.8970731808277811\n",
      "Model Trained and saved\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import joblib\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"C:\\\\Project_Guvi\\\\Capstone3\\\\Cleaned_Car_Dataset.csv\", low_memory=False)\n",
    "\n",
    "#load Preprocessing steps\n",
    "lable_encoders = joblib.load(\"C:\\\\Project_Guvi\\\\Capstone3\\\\Structured_data\\\\lable_encoded.pkl\")\n",
    "scalers = joblib.load(\"C:\\\\Project_Guvi\\\\Capstone3\\\\Structured_data\\\\scaled.pkl\")\n",
    "\n",
    "# Feature Engineering\n",
    "current_year = 2024\n",
    "df['car_age'] = current_year - df['modelYear']\n",
    "\n",
    "# Define features and target\n",
    "X = df[['ft', 'bt', 'km', 'transmission', 'ownerNo', 'oem', 'model', 'modelYear', 'variantName', 'City', 'mileage', 'seats','car_age']]\n",
    "y = df['price']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# \n",
    "rf_params = {\n",
    "    'n_estimators': [100, 200, 300, 400],\n",
    "    'max_depth': [10, 20, 30, 40],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "\n",
    "rf_random = RandomizedSearchCV(rf, rf_params, n_iter=20, cv=5, scoring='neg_mean_squared_error', random_state=42)\n",
    "rf_random.fit(X_train, y_train)\n",
    "\n",
    "best_rf = rf_random.best_estimator_\n",
    "\n",
    "# Cross validation\n",
    "rf_cv_scores = cross_val_score(rf, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "print(f'Random Forest CV Mean MSE: {-rf_cv_scores.mean():.5f}')\n",
    "\n",
    "# model prediction\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "# Model evaluation\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print the best model summary\n",
    "print(f'Random Forest - MSE: {mse}, MAE: {mae}, R²: {r2}')\n",
    "\n",
    "#save the trained model\n",
    "joblib.dump(best_rf,'C:\\\\Project_Guvi\\\\Capstone3\\\\cardekho_price_prediction_model.pkl')\n",
    "\n",
    "print(\"Model Trained and saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a13cfc-7d59-4ef1-9bc6-5af2060d3a33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
